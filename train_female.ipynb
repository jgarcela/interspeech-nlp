{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Interspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el CSV para male\n",
    "# data = pd.read_csv('data/data_female.csv')\n",
    "data = pd.read_csv('data/balanced_pseudo_female.csv')\n",
    "print(len(data))\n",
    "# Eliminar filas con valores nulos solo en la columna 'text'\n",
    "data = data.dropna(subset=['text'])\n",
    "#data.to_csv('data/data_female.csv', index=False)\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if balanced:\n",
    "    # Paso 1: Determinar el tamaño mínimo entre las clases\n",
    "    min_count = data['PseudoEmo'].value_counts().min()\n",
    "\n",
    "    # Paso 2: Submuestrear cada clase\n",
    "    data = data.groupby('PseudoEmo').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "\n",
    "    # Paso 3: Verificar el balance\n",
    "    print(data['PseudoEmo'].value_counts())\n",
    "    print(len(data))\n",
    "\n",
    "    data.to_csv('data/balanced_pseudo_female.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los datos en tres conjuntos basados en la columna 'Split_Set'\n",
    "train_df = data.loc[data['NewPartition'] == 'Train']\n",
    "dev_df = data.loc[data['NewPartition'] == 'Evaluation']\n",
    "test_df = data.loc[data['NewPartition'] == 'Test']\n",
    "\n",
    "# Verifica las primeras filas de cada conjunto\n",
    "print(\"Conjunto Train:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Conjunto Development:\")\n",
    "print(dev_df.head())\n",
    "\n",
    "print(\"Conjunto Test:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Establecer la semilla para garantizar reproducibilidad\n",
    "# seed = 42\n",
    "\n",
    "# # Seleccionar aleatoriamente las primeras N filas del conjunto de entrenamiento\n",
    "# train_df = train_df.sample(n=1000, random_state=seed)\n",
    "\n",
    "# # Seleccionar aleatoriamente las primeras N filas del conjunto de desarrollo\n",
    "# dev_df = dev_df.sample(n=250, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 8\n",
    "id2label = {\n",
    "        1: 'H',  # Happy\n",
    "        3: 'S',  # Sad\n",
    "        2: 'A',  # Angry\n",
    "        0: 'N',  # Neutral\n",
    "        5: 'U',  # Surprise\n",
    "        7: 'F',  # Fear\n",
    "        6: 'D',  # Disgust\n",
    "        4: 'C'   # Contempt\n",
    "    }\n",
    "label2id = {\n",
    "        'H': 1,  # Happy\n",
    "        'S': 2 ,  # Sad\n",
    "        'A': 3,  # Angry\n",
    "        'N': 4,  # Neutral\n",
    "        'U': 5,  # Surprise\n",
    "        'F': 6,  # Fear\n",
    "        'D': 7,  # Disgust\n",
    "        'C': 8   # Contempt\n",
    "    }\n",
    "print(f\"{id2label=}\")\n",
    "print(f\"{label2id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir EmoClass a valores numéricos si es necesario\n",
    "train_df['PseudoEmoNum'] = train_df['PseudoEmoNum'].astype(int)\n",
    "dev_df['PseudoEmoNum'] = dev_df['PseudoEmoNum'].astype(int)\n",
    "test_df['PseudoEmoNum'] = test_df['PseudoEmoNum'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if preprocessed == False:\n",
    "    # Modelo\n",
    "    model_ckpt = \"distilbert-base-uncased\"\n",
    "    # model_ckpt = \"distilroberta-base\"\n",
    "\n",
    "\n",
    "    # Cargar el tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    # Función para tokenizar los datos\n",
    "    def tokenize_function(examples):\n",
    "        # Verificar que estamos pasando una lista de textos\n",
    "        texts = examples['text']\n",
    "        return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Asegurarse de que 'train_df' y 'dev_df' son objetos Dataset de Hugging Face\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    dev_dataset = Dataset.from_pandas(dev_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "    # Tokenizamos ambos conjuntos de datos\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "    # Renombrar columna de labels\n",
    "    train_dataset = train_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n",
    "    dev_dataset = dev_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n",
    "    test_dataset = test_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "if preprocessed:\n",
    "    # Descargar recursos necesarios de NLTK\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Modelo\n",
    "    model_ckpt = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    # Stopwords en inglés (puedes cambiar el idioma si es necesario)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Inicializar el lematizador\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Función para limpiar y tokenizar\n",
    "    def preprocess_and_tokenize(examples):\n",
    "        processed_texts = []\n",
    "        for text in examples['text']:\n",
    "            # Convertir a minúsculas\n",
    "            text = text.lower()\n",
    "            # # Eliminar URLs\n",
    "            # text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "            # # Eliminar menciones y hashtags\n",
    "            # text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "            # # Eliminar caracteres especiales y puntuación\n",
    "            # text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "            # # Eliminar números\n",
    "            # text = re.sub(r\"\\d+\", \"\", text)\n",
    "            # # Eliminar palabras de parada\n",
    "            # words = text.split()\n",
    "            # words = [word for word in words if word not in stop_words]\n",
    "            # # Aplicar lematización\n",
    "            # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "            # # Unir palabras limpias\n",
    "            # text = \" \".join(words)\n",
    "            # Añadir texto procesado a la lista\n",
    "            processed_texts.append(text)\n",
    "        \n",
    "        # Tokenizar el texto limpio\n",
    "        return tokenizer(processed_texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Verificar que no haya valores nulos\n",
    "    train_df = train_df.dropna(subset=[\"text\", \"PseudoEmoNum\"])\n",
    "    dev_df = dev_df.dropna(subset=[\"text\", \"PseudoEmoNum\"])\n",
    "    test_df = test_df.dropna(subset=[\"text\", \"PseudoEmoNum\"])\n",
    "\n",
    "    # Convertir DataFrame a Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    dev_dataset = Dataset.from_pandas(dev_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    # Tokenizar los conjuntos de datos\n",
    "    train_dataset = train_dataset.map(preprocess_and_tokenize, batched=True, num_proc=4)\n",
    "    dev_dataset = dev_dataset.map(preprocess_and_tokenize, batched=True, num_proc=4)\n",
    "    test_dataset = test_dataset.map(preprocess_and_tokenize, batched=True, num_proc=4)\n",
    "\n",
    "    # Renombrar columna de labels\n",
    "    train_dataset = train_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n",
    "    dev_dataset = dev_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n",
    "    test_dataset = test_dataset.rename_column(\"PseudoEmoNum\", \"labels\")\n",
    "\n",
    "    # Inspeccionar un ejemplo procesado\n",
    "    print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "print(torch.__version__)  # Esto debería mostrarte la versión de PyTorch instalada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels, label2id=label2id, id2label=id2label).to(device)\n",
    "print(model.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Obtener reporte completo\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    \n",
    "    # Obtener la matriz de confusión\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    # Extraer métricas para cada clase y globales\n",
    "    metrics = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'macro_f1': report['macro avg']['f1-score'],\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        # 'weighted_precision': report['weighted avg']['precision'],\n",
    "        # 'weighted_recall': report['weighted avg']['recall'],\n",
    "        'confusion_matrix': conf_matrix.tolist()  # Convertir a lista para asegurarse de que es serializable si es necesario\n",
    "    }\n",
    "    \n",
    "    # # Añadir métricas específicas por clase si se requiere\n",
    "    # for label, scores in report.items():\n",
    "    #     if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "    #         metrics[f'{label}_precision'] = scores['precision']\n",
    "    #         metrics[f'{label}_recall'] = scores['recall']\n",
    "    #         metrics[f'{label}_f1'] = scores['f1-score']\n",
    "    #         metrics[f'{label}_support'] = scores['support']\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "logging_steps = len(train_dataset) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-female\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    # push_to_hub=True,\n",
    "    log_level=\"error\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(test_dataset)\n",
    "preds_metrics = preds_output.metrics\n",
    "preds_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = label2id.keys()\n",
    "conf_matrix = preds_metrics['test_confusion_matrix']\n",
    "\n",
    "# Crear el gráfico de la matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Agregar etiquetas\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "# Rótulos de valores dentro de cada celda\n",
    "conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "# Calcular el umbral\n",
    "thresh = conf_matrix.max() / 2\n",
    "for i, j in np.ndindex(conf_matrix.shape):\n",
    "    plt.text(\n",
    "        j, i, format(conf_matrix[i, j], 'd'),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if conf_matrix[i, j] > thresh else \"black\"\n",
    "    )\n",
    "\n",
    "# Etiquetas de los ejes\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
